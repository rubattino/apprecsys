{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EventDataRow(userId=65, itemId=551, ts=1421009506, latitude=47.237476, longitude=-122.530884, city=109, day_of_week=6, time_of_day=12, event_type=u'App_Opened'),\n",
       " EventDataRow(userId=65, itemId=821, ts=1421029924, latitude=47.237476, longitude=-122.530891, city=109, day_of_week=6, time_of_day=18, event_type=u'App_Opened')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "rawEventsRdd = sc.textFile(\"/home/mert/yahoo/events.txt\")\n",
    "EventDataRow = namedtuple(\"EventDataRow\", [\"userId\", \"itemId\", \"ts\", \"latitude\", \"longitude\", \"city\", \"day_of_week\", \"time_of_day\" , \"event_type\"])\n",
    "def parseRawData(line):\n",
    "    lineSplit = line.split(\"\\t\")\n",
    "    return EventDataRow(userId=lineSplit[0],\n",
    "                      itemId=lineSplit[1],\n",
    "                      ts=int(lineSplit[2]),\n",
    "                      latitude=float(lineSplit[3]),\n",
    "                      longitude=float(lineSplit[4]),\n",
    "                      city=lineSplit[5],\n",
    "                      day_of_week=int(lineSplit[6]),\n",
    "                      time_of_day=int(lineSplit[7]),\n",
    "                      event_type=lineSplit[-1],\n",
    "    )\n",
    "eventsRdd = sc.parallelize(rawEventsRdd.map(parseRawData).take(50000))\n",
    "#eventsRdd = rawEventsRdd.map(parseRawData).cache()\n",
    "userIdConversionDictionary = eventsRdd.map(lambda x: x.userId).distinct().zipWithIndex().collectAsMap()\n",
    "userIdConversionDictionaryBroadcast = sc.broadcast(userIdConversionDictionary)\n",
    "itemIdConversionDictionary = eventsRdd.map(lambda x: x.itemId).distinct().zipWithIndex().collectAsMap()\n",
    "itemIdConversionDictionaryBroadcast = sc.broadcast(itemIdConversionDictionary)\n",
    "cityConversionDictionary = eventsRdd.map(lambda x: x.city).distinct().zipWithIndex().collectAsMap()\n",
    "cityConversionDictionaryBroadcast = sc.broadcast(cityConversionDictionary)\n",
    "\n",
    "eventsConvertedRdd = eventsRdd.map(lambda x: EventDataRow(\n",
    "    userId=userIdConversionDictionaryBroadcast.value[x.userId],\n",
    "    itemId=itemIdConversionDictionaryBroadcast.value[x.itemId],\n",
    "    ts=x.ts,\n",
    "    latitude=x.latitude,\n",
    "    longitude=x.longitude,\n",
    "    city=cityConversionDictionaryBroadcast.value[x.city],\n",
    "    day_of_week=x.day_of_week,\n",
    "    time_of_day=x.time_of_day,\n",
    "    event_type=x.event_type\n",
    "    ))\n",
    "eventsConvertedRdd.take(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalRDD = eventsConvertedRdd.map(lambda x: [\n",
    "    x.userId,(\n",
    "    x.itemId,\n",
    "    x.ts,\n",
    "    x.latitude,\n",
    "    x.longitude,)\n",
    "    ])\n",
    "finalRDD.take(3)\n",
    "#groupData = map((lambda (x,y): (x, list(y))), sorted(finalRDD.groupByKey().collect()))\n",
    "#groupData = map((lambda (x,y): (x, sorted(list(y),key=lambda a: a[1]))), sorted(finalRDD.groupByKey()))\n",
    "groupData = finalRDD.groupByKey().map(lambda (x,y): (x, sorted(list(y),key=lambda a: a[1])))\n",
    "#groupData = sc.parallelize(groupData.take(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n",
    "def detectMovement(x):\n",
    "    data = x[1]\n",
    "    newData = [(data[0][0], data[0][1], data[0][2], data[0][3], 1)]\n",
    "    for i in xrange(1,len(data)):\n",
    "        event = data[i]\n",
    "        distance = haversine(event[3],event[2], data[i-1][3], data[i-1][2]) * 1000 #in meters\n",
    "        time_difference = event[1] - newData[i-1][1] #in seconds\n",
    "        moving = 1 #not available \n",
    "        if time_difference <= 300: #if 2 consecutive events are more than 300 seconds away, the movement is not available\n",
    "            velocity =  distance/time_difference if time_difference > 0 else -1\n",
    "            if velocity < 0:\n",
    "                moving = 1; #not available\n",
    "            elif velocity >= 0 and velocity <= 1:\n",
    "                moving = 2  #standing still\n",
    "            elif velocity <=2.4:\n",
    "                moving = 3 #walking spead\n",
    "            else:\n",
    "                moving = 4 #faster\n",
    "        newData.append((event[0],event[1],event[2],event[3], moving))\n",
    "    return (x[0], newData)\n",
    "    #return x\n",
    "#print haversine(elem[0][1][2][1],elem[0][1][1][1],elem[6][1][2][1],elem[6][1][1][1])\n",
    "groupData = groupData.map(detectMovement).cache()\n",
    "\n",
    "#groupData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 83, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1273, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-5-f26fe59d6127>\", line 82, in convertLocation\nTypeError: append() takes exactly one argument (5 given)\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f26fe59d6127>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlistGroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[0mfinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroupData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvertLocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m \u001b[0mfinal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/mert/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1277\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mert/spark-1.4.1-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions,\n\u001b[1;32m--> 897\u001b[1;33m                                           allowLocal)\n\u001b[0m\u001b[0;32m    898\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 83, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.py\", line 1273, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-5-f26fe59d6127>\", line 82, in convertLocation\nTypeError: append() takes exactly one argument (5 given)\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n",
    "\n",
    "def convertLocation(line):\n",
    "    listGroup = line[1]\n",
    "    workGroup = [x for x in listGroup if datetime.datetime.fromtimestamp(int(x[1])).hour >= 6 and  \n",
    "            datetime.datetime.fromtimestamp(int(x[1])).hour <= 18]\n",
    "    numNearLocation = []\n",
    "    i = 0\n",
    "    for x in workGroup:\n",
    "        numNearLocation.append(0);\n",
    "        for y in workGroup:\n",
    "            if haversine(x[3], x[2], y[3], y[2]) < 0.1:\n",
    "                numNearLocation[i] = numNearLocation[i] + 1\n",
    "        i = i + 1\n",
    "    if len(numNearLocation) > 0:\n",
    "        index_work = numNearLocation.index(max(numNearLocation))\n",
    "    else:\n",
    "        index_work = -1\n",
    "#     workGroup = [(x[0],x[1],x[2],x[3],1) if haversine(x[3], x[2], workGroup[index_work][3], workGroup[index_work][2]) < 0.1 \n",
    "#                  else (x[0],x[1],x[2],x[3],0) for x in workGroup]\n",
    "    #workGroup3 = [(x[0],x[1],0)  for x in workGroup if haversine(x[3], x[2], workGroup[index][3], workGroup[index][2]) >= 0.1]        \n",
    "    \n",
    "    homeGroup = [x for x in listGroup if datetime.datetime.fromtimestamp(int(x[1])).hour < 6 or\n",
    "            datetime.datetime.fromtimestamp(int(x[1])).hour > 18]\n",
    "    \n",
    "    numNearLocation = []\n",
    "    i = 0\n",
    "    for x in homeGroup:\n",
    "        numNearLocation.append(0);\n",
    "        for y in homeGroup:\n",
    "            if haversine(x[3], x[2], y[3], y[2]) < 0.1:\n",
    "                numNearLocation[i] = numNearLocation[i] + 1\n",
    "        i = i + 1\n",
    "    if len(numNearLocation) > 0:\n",
    "        index_home = numNearLocation.index(max(numNearLocation))\n",
    "    else:\n",
    "        index_home = -1\n",
    "#     homeGroup = [(x[0],x[1],x[2],x[3],2) if haversine(x[3], x[2], homeGroup[index_home][3], homeGroup[index_home][2]) < 0.1 \n",
    "#                  else (x[0],x[1],x[2],x[3],0) for x in homeGroup]\n",
    "    \n",
    "    if index_home != -1 and index_work != -1:\n",
    "        listGroup = [(x[0],x[1],x[4],1) if haversine(x[3], x[2], workGroup[index_work][3], workGroup[index_work][2]) < 0.01 \n",
    "                 else( \n",
    "                    (x[0],x[1],x[4],2) if haversine(x[3], x[2], homeGroup[index_home][3], homeGroup[index_home][2]) < 0.01\n",
    "                    else (x[0],x[1],x[4],3) \n",
    "                    )\n",
    "                 for x in listGroup]\n",
    "    else:\n",
    "        listGroup = [(x[0],x[1],x[4],3)\n",
    "                 for x in listGroup]\n",
    "    \n",
    "    \n",
    "    listGroup = [(x[0],x[2],x[3],1) if datetime.datetime.fromtimestamp(int(x[1])).hour >= 8 and\n",
    "                datetime.datetime.fromtimestamp(int(x[1])).hour <= 13\n",
    "                    else(\n",
    "                      (x[0],x[2],x[3],2) if datetime.datetime.fromtimestamp(int(x[1])).hour >= 13 and\n",
    "                        datetime.datetime.fromtimestamp(int(x[1])).hour <= 18\n",
    "                      else (x[0],x[2],x[3],3)\n",
    "                    )\n",
    "                for x in listGroup]\n",
    "    \n",
    "    newListGroup = []\n",
    "    for i in range(len(listGroup)):\n",
    "        if i == 0:\n",
    "            newListGroup.append(listGroup[i][0],listGroup[i][1],listGroup[i][2],listGroup[i][3],-1,)\n",
    "        else:\n",
    "            newListGroup.append(listGroup[i][0],listGroup[i][1],listGroup[i][2],listGroup[i][3],listGroup[i-1][0])\n",
    "    listGroup = newListGroup\n",
    "    #time = datetime.datetime.fromtimestamp(int(line[1][0][1]))\n",
    "    #line[1][1] = datetime.datetime.fromtimestamp(int(\"1284101485\")).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    #return line[0],(workGroup+homeGroup)[:20],listGroup[:20]#,len(workGroup+homeGroup),len(workGroup),len(homeGroup)\n",
    "    return line[0],listGroup\n",
    "final = groupData.map(convertLocation)\n",
    "final.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def topFiveSortedList(oldList, context, probability):\n",
    "    for x in range(0,len(oldList)-1):\n",
    "        if x == 0 and oldList[x][1] < probability:\n",
    "            oldList[x] = (context,probability)\n",
    "            if oldList[x+1][1] < oldList[x][1]:\n",
    "                temp = oldList[x+1] \n",
    "                oldList[x+1] = oldList[x]\n",
    "                oldList[x] = temp \n",
    "            else:\n",
    "                break\n",
    "        elif oldList[x+1][1] < oldList[x][1]:\n",
    "            temp = oldList[x+1] \n",
    "            oldList[x+1] = oldList[x]\n",
    "            oldList[x] = temp \n",
    "        else:\n",
    "            break\n",
    "    #return sorted(oldList,key=lambda x: -x[1])\n",
    "    return oldList      \n",
    "\n",
    "def remove_duplicates(values):\n",
    "    output = []\n",
    "    seen = set()\n",
    "    for value in values:\n",
    "        # If value has not been encountered yet,\n",
    "        # ... add it to both list and set.\n",
    "        if value not in seen:\n",
    "            output.append(value)\n",
    "            seen.add(value)\n",
    "    return output\n",
    "\n",
    "def bayesian(line):\n",
    "    listGroup = line[1]\n",
    "    #shuffle(listGroup)                  #shuffle the list\n",
    "    l = len(listGroup) \n",
    "    numTrain = l * 8 / 10\n",
    "    numTest = l - numTrain\n",
    "    trainList = listGroup[:numTrain]      #0.8 train set\n",
    "    testList = listGroup[numTrain:]       #0.2 test set\n",
    "    \n",
    "    #trainRDD = sc.parallelize(trainList).count()\n",
    "    newTestList = []\n",
    "    for t in testList:\n",
    "        context = [x for x in trainList if x[1]==t[1] and x[2]==t[2] and x[3]==t[3]]\n",
    "        numContext = float(len(context))\n",
    "        if numTrain != 0:\n",
    "            p_context = numContext/numTrain  #P(C1i, C2j, C3k)\n",
    "        else:\n",
    "            p_context = 0\n",
    "        p_app = [(-1,0),(-1,0),(-1,0),(-1,0),(-1,0)]\n",
    "        context_no_duplicate = remove_duplicates(context)\n",
    "        for c in context_no_duplicate:\n",
    "            appi = [x for x in trainList if x[0]==c[0]]\n",
    "            numAppi = float(len(appi))\n",
    "            if numTrain != 0:\n",
    "                p_appi = numAppi/numTrain\n",
    "            else:\n",
    "                p_appi = 0\n",
    "            contextAppi = [x for x in trainList if x[0]==c[0] and x[1]==c[1] and x[2]==c[2] and x[3]==c[3]]\n",
    "            if numAppi != 0:    #P(C1i, C2j, C3k | APPid)\n",
    "                p_contextAppi = len(contextAppi)/numAppi \n",
    "            else:\n",
    "                p_contextAppi = 0\n",
    "            if p_context != 0:  #P(APPid | C1i,C2j,C3k = P(C1i, C2j, C3k | APPid)  P(APPid) /P(C1i, C2j, C3k)\n",
    "                p = p_contextAppi * p_appi / p_context\n",
    "            else:\n",
    "                p = 0\n",
    "            p_app = topFiveSortedList(p_app,c[0],p)\n",
    "        p_app = sorted(p_app,key=lambda x: -x[1])\n",
    "        app_rec = map(lambda x:x[0],p_app[:5])\n",
    "        newTestList.append((t[0],app_rec))\n",
    "    scores = 0\n",
    "    numHit = 0\n",
    "\n",
    "    for t in newTestList:\n",
    "        if t[0] == t[1][0]:\n",
    "            scores = scores+1.0\n",
    "            numHit = numHit+1\n",
    "        elif t[0] == t[1][1]:\n",
    "            scores = scores+1.0\n",
    "            numHit = numHit+1\n",
    "        elif t[0] == t[1][2]:\n",
    "            scores = scores+1.0\n",
    "            numHit = numHit+1\n",
    "        elif t[0] == t[1][3]:\n",
    "            scores = scores+1.0\n",
    "            numHit = numHit+1\n",
    "        elif t[0] == t[1][4]:\n",
    "            scores = scores+1.0\n",
    "            numHit = numHit+1\n",
    "        else:\n",
    "            numHit = numHit+1\n",
    "    #scores = scores / numTest\n",
    "    if numHit != 0:\n",
    "        scores = scores / numHit\n",
    "    else:\n",
    "        scores = 0\n",
    "    #return newTestList[:20]\n",
    "    return scores\n",
    "result = final.map(bayesian)\n",
    "f = open('op2.txt','w')\n",
    "f.write(str(result.mean()))\n",
    "f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def topFiveSortedList(oldList, context, probability):\n",
    "    for x in range(0,len(oldList)-1):\n",
    "        if x == 0 and oldList[x][1] < probability:\n",
    "            oldList[x] = (context,probability)\n",
    "            if oldList[x+1][1] < oldList[x][1]:\n",
    "                temp = oldList[x+1] \n",
    "                oldList[x+1] = oldList[x]\n",
    "                oldList[x] = temp \n",
    "            else:\n",
    "                break\n",
    "        elif oldList[x+1][1] < oldList[x][1]:\n",
    "            temp = oldList[x+1] \n",
    "            oldList[x+1] = oldList[x]\n",
    "            oldList[x] = temp \n",
    "        else:\n",
    "            break\n",
    "    #return sorted(oldList,key=lambda x: -x[1])\n",
    "    return oldList      \n",
    "\n",
    "def remove_duplicates(values):\n",
    "    output = []\n",
    "    seen = set()\n",
    "    for value in values:\n",
    "        # If value has not been encountered yet,\n",
    "        # ... add it to both list and set.\n",
    "        if value not in seen:\n",
    "            output.append(value)\n",
    "            seen.add(value)\n",
    "    return output\n",
    "\n",
    "def bayesian(line):\n",
    "    listGroup = line[1]\n",
    "    #shuffle(listGroup)                  #shuffle the list\n",
    "    l = len(listGroup) \n",
    "    numTrain = l * 8 / 10\n",
    "    numTest = l - numTrain\n",
    "    trainList = listGroup[:numTrain]      #0.8 train set\n",
    "    testList = listGroup[numTrain:]       #0.2 test set\n",
    "    \n",
    "    #trainRDD = sc.parallelize(trainList).count()\n",
    "    newTestList = []\n",
    "    for t in testList:\n",
    "        context = [x for x in trainList if x[1]==t[1] and x[2]==t[2] and x[3]==t[3]]\n",
    "        numContext = float(len(context))\n",
    "        if numTrain != 0:\n",
    "            p_context = numContext/numTrain  #P(C1i, C2j, C3k)\n",
    "        else:\n",
    "            p_context = 0\n",
    "        p_app = [(-1,0),(-1,0),(-1,0),(-1,0),(-1,0)]\n",
    "        context_no_duplicate = remove_duplicates(context)\n",
    "        for c in context_no_duplicate:\n",
    "            appi = [x for x in trainList if x[0]==c[0]]\n",
    "            numAppi = float(len(appi))\n",
    "            if numTrain != 0:\n",
    "                p_appi = numAppi/numTrain\n",
    "            else:\n",
    "                p_appi = 0\n",
    "            contextAppi = [x for x in trainList if x[0]==c[0] and x[1]==c[1] and x[2]==c[2] and x[3]==c[3]]\n",
    "            if numAppi != 0:    #P(C1i, C2j, C3k | APPid)\n",
    "                p_contextAppi = len(contextAppi)/numAppi \n",
    "            else:\n",
    "                p_contextAppi = 0\n",
    "            if p_context != 0:  #P(APPid | C1i,C2j,C3k = P(C1i, C2j, C3k | APPid)  P(APPid) /P(C1i, C2j, C3k)\n",
    "                p = p_contextAppi * p_appi / p_context\n",
    "            else:\n",
    "                p = 0\n",
    "            p_app = topFiveSortedList(p_app,c[0],p)\n",
    "        p_app = sorted(p_app,key=lambda x: -x[1])\n",
    "        app_rec = map(lambda x:x[0],p_app[:5])\n",
    "        newTestList.append((t[0],app_rec))\n",
    "    scores = 0\n",
    "    numHit = 0\n",
    "\n",
    "    for t in newTestList:\n",
    "        if t[0] == t[1][0]:\n",
    "            scores = scores+1.0\n",
    "            numHit = numHit+1\n",
    "        elif t[0] == t[1][1]:\n",
    "            scores = scores+0.8\n",
    "            numHit = numHit+1\n",
    "        elif t[0] == t[1][2]:\n",
    "            scores = scores+0.6\n",
    "            numHit = numHit+1\n",
    "        elif t[0] == t[1][3]:\n",
    "            scores = scores+0.4\n",
    "            numHit = numHit+1\n",
    "        elif t[0] == t[1][4]:\n",
    "            scores = scores+0.2\n",
    "            numHit = numHit+1\n",
    "#         else:\n",
    "#             numHit = numHit+1\n",
    "    #scores = scores / numTest\n",
    "    if numHit != 0:\n",
    "        scores = scores / numHit\n",
    "    else:\n",
    "        scores = 0\n",
    "    #return newTestList[:20]\n",
    "    return scores\n",
    "result = final.map(bayesian)\n",
    "#result.mean()\n",
    "f = open('op.txt','w')\n",
    "f.write(str(result.mean()))\n",
    "f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def topFiveSortedList(oldList, context, probability):\n",
    "    for x in range(0,len(oldList)-1):\n",
    "        if x == 0 and oldList[x][1] < probability:\n",
    "            oldList[x] = (context,probability)\n",
    "            if oldList[x+1][1] < oldList[x][1]:\n",
    "                temp = oldList[x+1] \n",
    "                oldList[x+1] = oldList[x]\n",
    "                oldList[x] = temp \n",
    "            else:\n",
    "                break\n",
    "        elif oldList[x+1][1] < oldList[x][1]:\n",
    "            temp = oldList[x+1] \n",
    "            oldList[x+1] = oldList[x]\n",
    "            oldList[x] = temp \n",
    "        else:\n",
    "            break\n",
    "    #return sorted(oldList,key=lambda x: -x[1])\n",
    "    return oldList      \n",
    "\n",
    "def remove_duplicates(values):\n",
    "    output = []\n",
    "    seen = set()\n",
    "    for value in values:\n",
    "        # If value has not been encountered yet,\n",
    "        # ... add it to both list and set.\n",
    "        if value not in seen:\n",
    "            output.append(value)\n",
    "            seen.add(value)\n",
    "    return output\n",
    "\n",
    "def bayesian(line):\n",
    "    listGroup = line[1]\n",
    "    #shuffle(listGroup)                  #shuffle the list\n",
    "    l = len(listGroup) \n",
    "    numTrain = l * 8 / 10\n",
    "    numTest = l - numTrain\n",
    "    trainList = listGroup[:numTrain]      #0.8 train set\n",
    "    testList = listGroup[numTrain:]       #0.2 test set\n",
    "    \n",
    "    #trainRDD = sc.parallelize(trainList).count()\n",
    "    newTestList = []\n",
    "    for t in testList:\n",
    "        context = [x for x in trainList if x[1]==t[1] and x[2]==t[2] and x[3]==t[3]]\n",
    "        numContext = float(len(context))\n",
    "        if numTrain != 0:\n",
    "            p_context = numContext/numTrain  #P(C1i, C2j, C3k)\n",
    "        else:\n",
    "            p_context = 0\n",
    "        p_app = [(-1,0),(-1,0),(-1,0),(-1,0),(-1,0)]\n",
    "        context_no_duplicate = remove_duplicates(context)\n",
    "        for c in context_no_duplicate:\n",
    "            appi = [x for x in trainList if x[0]==c[0]]\n",
    "            numAppi = float(len(appi))\n",
    "            if numTrain != 0:\n",
    "                p_appi = numAppi/numTrain\n",
    "            else:\n",
    "                p_appi = 0\n",
    "            contextAppi = [x for x in trainList if x[0]==c[0] and x[1]==c[1] and x[2]==c[2] and x[3]==c[3]]\n",
    "            if numAppi != 0:    #P(C1i, C2j, C3k | APPid)\n",
    "                p_contextAppi = len(contextAppi)/numAppi \n",
    "            else:\n",
    "                p_contextAppi = 0\n",
    "            if p_context != 0:  #P(APPid | C1i,C2j,C3k = P(C1i, C2j, C3k | APPid)  P(APPid) /P(C1i, C2j, C3k)\n",
    "                p = p_contextAppi * p_appi / p_context\n",
    "            else:\n",
    "                p = 0\n",
    "            p_app = topFiveSortedList(p_app,c[0],p)\n",
    "        p_app = sorted(p_app,key=lambda x: -x[1])\n",
    "        app_rec = map(lambda x:x[0],p_app[:5])\n",
    "        newTestList.append((t[0],app_rec))\n",
    "    scores = 0\n",
    "    numHit = 0\n",
    "\n",
    "    for t in newTestList:\n",
    "        if t[0] == t[1][0]:\n",
    "            scores = scores+1.0\n",
    "            numHit = numHit+1\n",
    "        elif t[0] == t[1][1]:\n",
    "            scores = scores+0.8\n",
    "            numHit = numHit+1\n",
    "        elif t[0] == t[1][2]:\n",
    "            scores = scores+0.6\n",
    "            numHit = numHit+1\n",
    "        elif t[0] == t[1][3]:\n",
    "            scores = scores+0.4\n",
    "            numHit = numHit+1\n",
    "        elif t[0] == t[1][4]:\n",
    "            scores = scores+0.2\n",
    "            numHit = numHit+1\n",
    "        else:\n",
    "            numHit = numHit+1\n",
    "    #scores = scores / numTest\n",
    "    if numHit != 0:\n",
    "        scores = scores / numHit\n",
    "    else:\n",
    "        scores = 0\n",
    "    #return newTestList[:20]\n",
    "    return scores\n",
    "result = final.map(bayesian)\n",
    "f = open('output.txt','w')\n",
    "f.write(str(result.mean()))\n",
    "f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = sc.parallelize([1, 1, 1, 1])\n",
    "\n",
    "f = open('output.txt','w')\n",
    "f.write(str(result.mean()))\n",
    "f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a56d6c27f332>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not callable"
     ]
    }
   ],
   "source": [
    "a = [1, 2]\n",
    "b = [1, 2]\n",
    "(x in a for x in b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
