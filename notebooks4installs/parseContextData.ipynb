{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'001e6d8e-cbe7-4374-8c38-f37962a457e9\\tair.com.buffalo_studios.bingorush2\\t1420974077\\t47.237461\\t-122.530907\\tTacoma\\t6\\t3\\tApp_Installed\\tinstall',\n",
       " u'009ef415-eb39-4f3e-8714-2b22a6490fbb\\tcom.pandora.android\\t1421228526\\t37.353397\\t-121.944977\\tSanta Clara\\t2\\t1\\tApp_Installed\\tinstall',\n",
       " u'01fb3e32-5e7f-498d-aa51-aa8eabdb12ac\\tcom.fractal360.go.launcherex.theme.gfl\\t1421543961\\t33.842270\\t-84.211357\\tTucker\\t5\\t20\\tApp_Installed\\tinstall',\n",
       " u'01fb3e32-5e7f-498d-aa51-aa8eabdb12ac\\tcom.jb.gokeyboard.theme.el\\t1421543917\\t33.842270\\t-84.211357\\tTucker\\t5\\t20\\tApp_Installed\\tinstall',\n",
       " u'01fb3e32-5e7f-498d-aa51-aa8eabdb12ac\\tcom.jb.emoji.gokeyboard\\t1421544179\\t33.84229\\t-84.211235\\tTucker\\t5\\t20\\tApp_Installed\\tinstall']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "execfile(\"../script/utils.py\")\n",
    "eventsPath = os.environ[\"YAHOO_DATA\"]\n",
    "installsRawRdd = sc.textFile(eventsPath + \"/installs.txt\")\n",
    "#splitedRdd = splitedRdd.map(parseContextData2)\n",
    "#splitedRdd.take(1)\n",
    "#splitedRdd = sc.parallelize(splitedRdd.take(2000))\n",
    "installsRawRdd.take(5)\n",
    "#(uid,[[train],[test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EventDataRow(userId=u'001e6d8e-cbe7-4374-8c38-f37962a457e9', itemId=u'air.com.buffalo_studios.bingorush2', ts=1420974077, latitude=47.237461, longitude=-122.530907, city=u'Tacoma', day_of_week=6, time_of_day=3, event_type=u'install'),\n",
       " EventDataRow(userId=u'009ef415-eb39-4f3e-8714-2b22a6490fbb', itemId=u'com.pandora.android', ts=1421228526, latitude=37.353397, longitude=-121.944977, city=u'Santa Clara', day_of_week=2, time_of_day=1, event_type=u'install'),\n",
       " EventDataRow(userId=u'01fb3e32-5e7f-498d-aa51-aa8eabdb12ac', itemId=u'com.fractal360.go.launcherex.theme.gfl', ts=1421543961, latitude=33.84227, longitude=-84.211357, city=u'Tucker', day_of_week=5, time_of_day=20, event_type=u'install'),\n",
       " EventDataRow(userId=u'01fb3e32-5e7f-498d-aa51-aa8eabdb12ac', itemId=u'com.jb.gokeyboard.theme.el', ts=1421543917, latitude=33.84227, longitude=-84.211357, city=u'Tucker', day_of_week=5, time_of_day=20, event_type=u'install'),\n",
       " EventDataRow(userId=u'01fb3e32-5e7f-498d-aa51-aa8eabdb12ac', itemId=u'com.jb.emoji.gokeyboard', ts=1421544179, latitude=33.84229, longitude=-84.211235, city=u'Tucker', day_of_week=5, time_of_day=20, event_type=u'install')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EventDataRow = namedtuple(\"EventDataRow\",\n",
    "                          [\"userId\", \"itemId\", \"ts\", \"latitude\", \"longitude\", \"city\", \"day_of_week\", \"time_of_day\",\n",
    "                           \"event_type\"])\n",
    "def parseRawData(line):\n",
    "    lineSplit = line.split(\"\\t\")\n",
    "    return EventDataRow(userId=lineSplit[0],\n",
    "                        itemId=lineSplit[1],\n",
    "                        ts=int(lineSplit[2]),\n",
    "                        latitude=float(lineSplit[3]),\n",
    "                        longitude=float(lineSplit[4]),\n",
    "                        city=lineSplit[5],\n",
    "                        day_of_week=int(lineSplit[6]),\n",
    "                        time_of_day=int(lineSplit[7]),\n",
    "                        event_type=lineSplit[-1],)\n",
    "eventsRdd = installsRawRdd.map(parseRawData).cache()\n",
    "eventsRdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 21, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-17-98e6c669dbd2>\", line 5, in parseRawData\nAttributeError: 'EventDataRow' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:242)\n\tat org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)\n\tat org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d6cce7149976>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0meventsRdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparseInstallsRawRdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparseRawData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0muserIdConversionDictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meventsRdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muserId\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzipWithIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0muserIdConversionDictionaryBroadcast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserIdConversionDictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mitemIdConversionDictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meventsRdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemId\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzipWithIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mitemIdConversionDictionaryBroadcast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitemIdConversionDictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mert/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mzipWithIndex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2065\u001b[0m         \u001b[0mstarts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2066\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2067\u001b[1;33m             \u001b[0mnums\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2068\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2069\u001b[0m                 \u001b[0mstarts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstarts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnums\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mert/spark-1.4.1-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \"\"\"\n\u001b[0;32m    756\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 21, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/mert/spark-1.4.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-17-98e6c669dbd2>\", line 5, in parseRawData\nAttributeError: 'EventDataRow' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:242)\n\tat org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)\n\tat org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "eventsRdd = parseInstallsRawRdd.map(parseRawData).cache()\n",
    "userIdConversionDictionary = eventsRdd.map(lambda x: x.userId).distinct().zipWithIndex().collectAsMap()\n",
    "userIdConversionDictionaryBroadcast = sc.broadcast(userIdConversionDictionary)\n",
    "itemIdConversionDictionary = eventsRdd.map(lambda x: x.itemId).distinct().zipWithIndex().collectAsMap()\n",
    "itemIdConversionDictionaryBroadcast = sc.broadcast(itemIdConversionDictionary)\n",
    "cityConversionDictionary = eventsRdd.map(lambda x: x.city).distinct().zipWithIndex().collectAsMap()\n",
    "cityConversionDictionaryBroadcast = sc.broadcast(cityConversionDictionary)\n",
    "eventsConvertedRdd = eventsRdd.map(lambda x: EventDataRow(\n",
    "    userId=userIdConversionDictionaryBroadcast.value[x.userId],\n",
    "    itemId=itemIdConversionDictionaryBroadcast.value[x.itemId],\n",
    "    ts=x.ts,\n",
    "    latitude=x.latitude,\n",
    "    longitude=x.longitude,\n",
    "    city=cityConversionDictionaryBroadcast.value[x.city],\n",
    "    day_of_week=x.day_of_week,\n",
    "    time_of_day=x.time_of_day,\n",
    "     event_type=x.event_type\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    finalRDD = eventsConvertedRdd.map(lambda x: [\n",
    "        x.userId, (\n",
    "            x.itemId,\n",
    "            x.ts,\n",
    "            x.latitude,\n",
    "            x.longitude,)\n",
    "    ])\n",
    "\n",
    "    groupData = map((lambda (x, y): (x, sorted(list(y), key=lambda a: a[1]))), sorted(finalRDD.groupByKey().collect()))\n",
    "\n",
    "    def detectMovement(x):\n",
    "        data = x[1]\n",
    "        newData = [(data[0][0], data[0][1], data[0][2], data[0][3], 1)]\n",
    "        for i in xrange(1, len(data)):\n",
    "            event = data[i]\n",
    "            distance = haversine(event[3], event[2], data[i - 1][3], data[i - 1][2]) * 1000  #in meters\n",
    "            time_difference = event[1] - newData[i - 1][1]  #in seconds\n",
    "            moving = 1  #not available\n",
    "            if time_difference <= 300:  #if 2 consecutive events are more than 300 seconds away, the movement is not available\n",
    "                velocity = distance / time_difference if time_difference > 0 else -1\n",
    "                if velocity < 0:\n",
    "                    moving = 1;  #not available\n",
    "                elif velocity >= 0 and velocity <= 1:\n",
    "                    moving = 2  #standing still\n",
    "                elif velocity <= 2.4:\n",
    "                    moving = 3  #walking spead\n",
    "                else:\n",
    "                    moving = 4  #faster\n",
    "            newData.append((event[0], event[1], event[2], event[3], moving))\n",
    "        return (x[0], newData)\n",
    "\n",
    "    groupData = sc.parallelize(groupData).map(detectMovement).cache()\n",
    "\n",
    "    def convertLocation(line):\n",
    "        listGroup = line[1]\n",
    "        workGroup = [x for x in listGroup if datetime.datetime.fromtimestamp(int(x[1])).hour >= 6 and\n",
    "                     datetime.datetime.fromtimestamp(int(x[1])).hour <= 18]\n",
    "        numNearLocation = []\n",
    "        i = 0\n",
    "        for x in workGroup:\n",
    "            numNearLocation.append(0);\n",
    "            for y in workGroup:\n",
    "                if haversine(x[3], x[2], y[3], y[2]) < 0.1:\n",
    "                    numNearLocation[i] = numNearLocation[i] + 1\n",
    "            i = i + 1\n",
    "        index_work = numNearLocation.index(max(numNearLocation))\n",
    "        #     workGroup = [(x[0],x[1],x[2],x[3],1) if haversine(x[3], x[2], workGroup[index_work][3], workGroup[index_work][2]) < 0.1\n",
    "        #                  else (x[0],x[1],x[2],x[3],0) for x in workGroup]\n",
    "        #workGroup3 = [(x[0],x[1],0)  for x in workGroup if haversine(x[3], x[2], workGroup[index][3], workGroup[index][2]) >= 0.1]\n",
    "\n",
    "        homeGroup = [x for x in listGroup if datetime.datetime.fromtimestamp(int(x[1])).hour < 6 or\n",
    "                     datetime.datetime.fromtimestamp(int(x[1])).hour > 18]\n",
    "\n",
    "        numNearLocation = []\n",
    "        i = 0\n",
    "        for x in homeGroup:\n",
    "            numNearLocation.append(0);\n",
    "            for y in homeGroup:\n",
    "                if haversine(x[3], x[2], y[3], y[2]) < 0.1:\n",
    "                    numNearLocation[i] = numNearLocation[i] + 1\n",
    "            i = i + 1\n",
    "        index_home = numNearLocation.index(max(numNearLocation))\n",
    "        #     homeGroup = [(x[0],x[1],x[2],x[3],2) if haversine(x[3], x[2], homeGroup[index_home][3], homeGroup[index_home][2]) < 0.1\n",
    "        #                  else (x[0],x[1],x[2],x[3],0) for x in homeGroup]\n",
    "\n",
    "        listGroup = [\n",
    "            (x[0], x[1], x[4], 1) if haversine(x[3], x[2], workGroup[index_work][3], workGroup[index_work][2]) < 0.01\n",
    "            else(\n",
    "                (x[0], x[1], x[4], 2) if haversine(x[3], x[2], homeGroup[index_home][3],\n",
    "                                                   homeGroup[index_home][2]) < 0.01\n",
    "                else (x[0], x[1], x[4], 3)\n",
    "            )\n",
    "            for x in listGroup]\n",
    "\n",
    "        listGroup = [(x[0], x[2], x[3], 1) if datetime.datetime.fromtimestamp(int(x[1])).hour >= 8 and\n",
    "                                              datetime.datetime.fromtimestamp(int(x[1])).hour <= 13\n",
    "                     else(\n",
    "            (x[0], x[2], x[3], 2) if datetime.datetime.fromtimestamp(int(x[1])).hour >= 13 and\n",
    "                                     datetime.datetime.fromtimestamp(int(x[1])).hour <= 18\n",
    "            else (x[0], x[2], x[3], 3)\n",
    "        )\n",
    "                     for x in listGroup]\n",
    "\n",
    "\n",
    "        #time = datetime.datetime.fromtimestamp(int(line[1][0][1]))\n",
    "        #line[1][1] = datetime.datetime.fromtimestamp(int(\"1284101485\")).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        #return line[0],(workGroup+homeGroup)[:20],listGroup[:20]#,len(workGroup+homeGroup),len(workGroup),len(homeGroup)\n",
    "        return line[0], listGroup\n",
    "\n",
    "    return groupData.map(convertLocation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
